
# Project Title

ğŸ• Next Sentence Prediction using Custom LSTM Model


## ğŸ“– Overview

Next Sentence Prediction (NSP) is the task of generating or identifying the most plausible next sentence given a previous sentence or context. Unlike large models like GPT-2, this project uses a custom LSTM network trained from scratch using a domain-specific dataset.

This approach is useful in resource-constrained environments and can be fine-tuned for domain-specific applications like chatbots, auto-completion, or content extension.
## ğŸ“‚ Dataset

The dataset used is a plain text file named pizza.txt, containing pizza-related content in natural language.

It is tokenized into sentences and then into n-gram sequences for model training.
## ğŸ§  Model Architecture

Tokenizer: Keras Tokenizer for word-level sequences

Embedding Layer: Converts word indices to dense vectors

LSTM Layer: Learns context and sequence patterns

Dense Layer: Outputs a probability distribution over vocabulary

Pythone code :

model = Sequential()
model.add(Embedding(total_words, 10, input_length=max_sequence_len-1))
model.add(LSTM(128))
model.add(Dense(total_words, activation='softmax'))


Loss Function: Categorical Crossentropy

Optimizer: Adam

Training Epochs: 100 (can be tuned for more or number)
## ğŸ› ï¸ How to Run

Clone using:

git clone https://github.com/Aditya5437/IBM-GenAI-Final_project.git


Required Libraries:

pip install tensorflow numpy regex


Make sure pizza.txt is in the same directory.

Run Jupyter Notebook:

jupyter notebook NLP_Project_2.ipynb


Provide a seed sentence (e.g., "I love eating") and observe the generated next words.
## ğŸ§ª Sample Results

Input: I ordered a pizza with
Output: extra cheese and mushrooms.

Input: The oven was preheated to
Output: 425 degrees Fahrenheit.

These are realistic predictions generated by the LSTM model trained on the pizza domain.
## ğŸ“š References

TensorFlow Keras Documentation

Text Generation with LSTM

Custom training corpus: pizza.txt
